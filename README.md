# Capabilities of GPT-5 on Multimodal Medical Reasoning


This repository contains the evaluation code and GPT-5's output for "Capabilities of GPT-5 on Multimodal Medical Reasoning".


> **<p align="justify"> Abstract:** *Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5â€™s ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. These findings indicate that GPT-5 achieves a decisive leap from parity with human experts to super-human performance in controlled multimodal medical reasoning benchmarks, with significant implications for future clinical decision-support systems.* </p>



## Citation

If you find our work and this repository helpful, please consider giving our repo a star and citing our paper as follows:

```bibtex
XXX
```

## Acknowledgement
Our code repository is mainly built on [MedXpertQA](https://github.com/TsinghuaC3I/MedXpertQA). We thank the authors for releasing their code. 
